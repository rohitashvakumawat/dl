---Statement 21  
Implement a CNN on Tomato dataset using batch sizes of 32 and 64 separately. Keep the learning 
rate fixed at 0.0001 and compare results.

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling

# 1. Dataset path
dataset_path = "tomato_dataset"  # Replace with your folder path

# 2. Function to load dataset with specific batch size
def load_dataset(batch_size):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        dataset_path,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=(128, 128),
        batch_size=batch_size)

    val_ds = tf.keras.utils.image_dataset_from_directory(
        dataset_path,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=(128, 128),
        batch_size=batch_size)

    train_ds = train_ds.map(lambda x, y: (Rescaling(1./255)(x), y))
    val_ds = val_ds.map(lambda x, y: (Rescaling(1./255)(x), y))

    return train_ds, val_ds

# 3. CNN model builder
def build_model():
    model = tf.keras.Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
        MaxPooling2D(),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(),
        Flatten(),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dense(2, activation='softmax')  # 2 classes: healthy and diseased
    ])
    return model

# 4. Function to compile, train, and evaluate
def train_and_evaluate(batch_size):
    print(f"\nTraining with Batch Size: {batch_size}")
    train_ds, val_ds = load_dataset(batch_size)
    
    model = build_model()
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    
    history = model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)
    
    loss, acc = model.evaluate(val_ds, verbose=0)
    print(f"Validation Accuracy: {acc:.2f}, Loss: {loss:.4f}")
    
    return history

# 5. Run training for both batch sizes
history_32 = train_and_evaluate(batch_size=32)
history_64 = train_and_evaluate(batch_size=64)

# 6. Plot comparison of accuracy
plt.plot(history_32.history['val_accuracy'], label='Batch 32 Accuracy')
plt.plot(history_64.history['val_accuracy'], label='Batch 64 Accuracy')
plt.title("Validation Accuracy Comparison")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# 7. Plot comparison of loss
plt.plot(history_32.history['val_loss'], label='Batch 32 Loss')
plt.plot(history_64.history['val_loss'], label='Batch 64 Loss')
plt.title("Validation Loss Comparison")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
